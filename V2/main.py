# main.py

# ================================
# IMPORT DES LIBRAIRIES
# ================================
import praw  # Pour interagir avec l'API Reddit
import urllib.request  # Pour r√©cup√©rer des donn√©es depuis ArXiv
import certifi, ssl  # Pour g√©rer les certificats SSL de mani√®re s√©curis√©e
import xmltodict  # Pour parser les flux XML (ArXiv)
from datetime import datetime  # Pour manipuler les dates
from Corpus import Corpus  # Classe pour g√©rer un corpus de documents
from RedditDocument import RedditDocument  # Classe repr√©sentant un post Reddit
from ArxivDocument import ArxivDocument  # Classe repr√©sentant un document ArXiv
from SearchEngine import SearchEngine  # Classe pour rechercher dans le corpus
import pandas as pd  # Pour manipuler des tableaux de donn√©es
import re  # Pour le traitement de texte (regex)
from Document import Document  # Classe g√©n√©rique pour un document

# ================================
# INITIALISATION DU CORPUS
# ================================
corpus = Corpus("Corpus_Reddit_Arxiv")  # Cr√©ation d'un corpus vide nomm√© "Corpus_Reddit_Arxiv"

# ================================
# CONNEXION √Ä REDDIT
# ================================
reddit = praw.Reddit(
    client_id='vc2Hro3ys8p9rqCG6bHeAg',  # ID client Reddit
    client_secret='gedHKB0SBBkm9H2vEwhnsCPw5UykVg',  # Secret client
    user_agent='WebScraping'  # Nom de l'application pour Reddit
)

print("\n--- R√©cup√©ration des posts Reddit ---")
ml_subreddit = reddit.subreddit('MachineLearning')  # Acc√®s au subreddit MachineLearning

# Parcours des 50 posts les plus populaires
for post in ml_subreddit.hot(limit=50):
    titre = post.title.replace("\n", " ")  # Nettoyage du titre
    auteur = str(post.author) if post.author else "Inconnu"  # Gestion auteur inconnu
    date_pub = datetime.fromtimestamp(post.created_utc)  # Conversion timestamp en date
    url = f"https://www.reddit.com{post.permalink}"  # URL du post
    texte = post.selftext if post.selftext else post.title  # Texte du post
    nb_comments = post.num_comments  # Nombre de commentaires

    # Cr√©ation d'un objet RedditDocument et ajout au corpus
    doc = RedditDocument(titre, auteur, date_pub, url, texte, nb_comments)
    corpus.add_document(doc)

# ================================
# R√âCUP√âRATION DES PUBLICATIONS ARXIV
# ================================
print("\n--- R√©cup√©ration des publications ArXiv ---")
context = ssl.create_default_context(cafile=certifi.where())  # Contexte SSL s√©curis√©
url = 'http://export.arxiv.org/api/query?search_query=all:Machine&start=0&max_results=50'

# R√©cup√©ration et parsing du flux XML
data = urllib.request.urlopen(url, context=context).read().decode('utf-8')
dict_data = xmltodict.parse(data)
entries = dict_data['feed']['entry']

# Parcours des publications
for entry in entries:
    titre = entry['title'].replace("\n", " ")  # Nettoyage du titre

    # Gestion d'un ou plusieurs auteurs
    if isinstance(entry['author'], list):
        auteur = entry['author'][0]['name']  # Auteur principal
        coauthors = [a['name'] for a in entry['author'][1:]]  # Co-auteurs
    else:
        auteur = entry['author']['name']
        coauthors = []

    date_pub = datetime.fromisoformat(entry['published'].replace('Z', '+00:00'))  # Conversion date
    url = entry['id']  # URL du document
    texte = entry['summary'].replace("\n", " ")  # R√©sum√© nettoy√©

    # Cr√©ation d'un objet ArxivDocument et ajout au corpus
    doc = ArxivDocument(titre, auteur, date_pub, url, texte, coauthors)
    corpus.add_document(doc)

# ================================
# AFFICHAGE DU CORPUS
# ================================
print("\n=== Aper√ßu du corpus ===")
print(corpus)  # Affiche un r√©sum√© du corpus
corpus.afficher_par_date(20)  # Affiche les 20 documents les plus r√©cents
corpus.afficher_par_titre(20)  # Affiche les 20 premiers titres

# ================================
# SAUVEGARDE ET RECHARGEMENT
# ================================
corpus.save("corpus.csv")  # Sauvegarde du corpus
nouveau_corpus = Corpus("Corpus_Recharge")  # Cr√©ation d'un nouveau corpus vide
nouveau_corpus.load("corpus.csv")  # Recharge le corpus sauvegard√©

print("\n‚úÖ Corpus recharg√© :")
print(nouveau_corpus)

# ================================
# RECHERCHE PAR AUTEUR
# ================================
nom_recherche = input("\nEntrez le nom d'un auteur : ")

if nom_recherche in nouveau_corpus.authors:
    auteur_obj = nouveau_corpus.authors[nom_recherche]
    print(f"\nAuteur trouv√© : {auteur_obj.name}")
    print(f"Nombre de documents : {auteur_obj.nb_docs}")

    # Calcul de la taille moyenne des documents
    longueurs = [len(doc.texte.split()) for doc in auteur_obj.production.values()]
    moyenne = sum(longueurs) / len(longueurs) if longueurs else 0
    print(f"Taille moyenne des documents : {moyenne:.2f} mots")
else:
    print(f"Auteur '{nom_recherche}' non trouv√© dans le corpus.")

# ================================
# RECHERCHE DE MOTS-CL√âS DANS LE CORPUS
# ================================
mot_test = input("\nüîç Entrez un mot-cl√© √† rechercher dans le corpus : ")

print("\n--- Test de la m√©thode search() ---")
resultats_search = nouveau_corpus.search(mot_test, contexte=60)  # Recherche avec contexte de 60 caract√®res
print(f"\nNombre de passages trouv√©s : {len(resultats_search)}")

print("\n--- Test de la m√©thode concorde() ---")
df_concorde = nouveau_corpus.concorde(mot_test, contexte=60)  # Concordancier
print(f"\nNombre de concordances trouv√©es : {len(df_concorde)}")

if not df_concorde.empty:
    df_concorde.to_csv("concordancier.csv", sep='\t', index=False)  # Sauvegarde du concordancier
    print("\n‚úÖ Concordancier sauvegard√© dans 'concordancier.csv'")

# ================================
# STATISTIQUES DU CORPUS
# ================================
print("\n=== Statistiques sur le corpus ===")
freq = nouveau_corpus.stats(20)  # Top 20 mots les plus fr√©quents
freq.to_csv("stats_vocab.csv", sep='\t', index=False)
print("\n‚úÖ Statistiques sauvegard√©es dans 'stats_vocab.csv'")

# ================================
# MOTEUR DE RECHERCHE TF + TF-IDF
# ================================
print("\n=== Construction du moteur de recherche (TF + TF-IDF) ===")
engine = SearchEngine(nouveau_corpus)
print("‚úÖ Moteur de recherche cr√©√©.")

query = input("\nüîé Entrez des mots-cl√©s pour le moteur TF-IDF : ")
df_results = engine.search(query, k=10)  # Recherche top 10
print("\n=== R√©sultats du moteur de recherche ===")
print(df_results)

